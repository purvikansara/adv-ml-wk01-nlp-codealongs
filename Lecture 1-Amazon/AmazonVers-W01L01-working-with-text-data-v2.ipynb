{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbc1ae31-d891-446e-a2f5-f99252269668",
   "metadata": {},
   "source": [
    "# Advanced ML Week 1, Lecture 1: Working with and Preparing Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c7733b-910b-48af-99a9-150b6bfee49d",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "\n",
    "- This is a modified version of our in-class notebook from lecture 01 with Twitter Sentiment analysis.\n",
    "- The dataset has been replaced with a subset of amazon product reviews.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c4f396-ab59-44d3-9828-7929a0695a77",
   "metadata": {},
   "source": [
    "### New Sections/Content\n",
    "- See Prepare-Amazon-Reviews-Subset-csv.ipynb for selection of subset brand.\n",
    "- [✨Removing HTML with Regex](#regex)\n",
    "- Data Introduction (below):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a22af81-227d-4a7e-b7dd-4a92ab8ca958",
   "metadata": {},
   "source": [
    "### Amazon Data Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d43a156-1d6d-4d70-856a-22daa3d93271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "with open(\"../Data-AmazonReviews/Amazon Product Reviews.md\") as f:\n",
    "    info = f.read()\n",
    "\n",
    "display(Markdown(info))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9973bac9-f64b-4c1b-ab7d-c0e761f86d9c",
   "metadata": {},
   "source": [
    "<!-- In this notebook we will be preparing Twitter (X) Tweets for sentiment analysis.  Sentiment analysis is a common text classification challenge to determine whether a text is positive or negative.  \n",
    "\n",
    "This is useful for companies that want to analyze large numbers of documents, tweets, reviews, etc., to determine public sentiment about a product or service.\n",
    "\n",
    "The data was originally gathered from Twitter (now X) and hand-labeled.  Of course there will be some human bias in the labeling.  It was downloaded from Kaggle at this site: [Kaggle Twitter Tweets Sentiment Dataset](https://www.kaggle.com/datasets/yasserh/twitter-tweets-sentiment-dataset/)\n",
    "\n",
    "There are 3 classes: positive, negative, and neutral. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6c695c-5cfd-4069-aacf-dacd0457e3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary packages\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c0645a-3d8a-41f9-a678-23b5862ac8a2",
   "metadata": {},
   "source": [
    "# Load the Data\n",
    "\n",
    "We will load our **corpus** of Amazon Reviews for Hoover products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdf2423-787a-4089-b601-85edee02ca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data-AmazonReviews/amazon-reviews-home-kitchen_hoover.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15206203-a97b-448d-b5e9-a95c511ec56a",
   "metadata": {},
   "source": [
    "# Some light EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e60c31-28a8-48a2-a150-cc2eb9ed3f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c869dc5e-8f79-4634-a3af-226522d4c641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd5c31-8f5e-4170-b941-57bdeb9459c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['reviewText','summary'])\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3e96d1-975b-493b-9551-9e242871df7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30895698-db7d-49a2-b920-a083f81f9921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New\n",
    "df = df.drop_duplicates()\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fac6618-26a8-4fcf-a8ef-5aaebcd42901",
   "metadata": {},
   "source": [
    "# Some Light Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498ca912-8eb6-44f2-af0c-d5f7da11198d",
   "metadata": {},
   "source": [
    "### ✨ New Cleaning for Amazon:\n",
    "\n",
    "- The reviews are split into 2 parts. The reviewText, which is the majority of the review, and the summary, which is a 1-line summary of the review (that often includes the actual rating: e.g., \"Fours stars- best vacuum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4fc876-76fb-43e5-9af6-9c8ad03b9238",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text-raw'] = df['summary'] + \": \" + df['reviewText']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55bfece-7c3e-43df-be09-7ccc75e87817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67444f09-6f17-4b1c-9304-f39735be5177",
   "metadata": {},
   "source": [
    "#### Confirming Which Columns to Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b0e2e0-50fa-4446-9eef-38a50ae5d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new\n",
    "df['brand'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47231a2f-4b1e-494f-bec1-537823800db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new\n",
    "df['title'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64be5747-3e29-4745-ab38-c25e09255dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['brand', 'reviewText','summary','title'])\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbbcd7d-1e2f-4d44-bb98-43ece33d3ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922b390c-c90c-46d4-be9f-061e8a66f2f2",
   "metadata": {},
   "source": [
    "# Some More EDA\n",
    "Let's look at some aspects of this text.\n",
    "* What do the **documents** look like?\n",
    "* How long do the tend to be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0656c034-5ce8-4214-8147-a18f1150699f",
   "metadata": {},
   "source": [
    "## View some sample tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2e8add-e130-49ac-8ffa-9dcb7d59568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Expand how many characters pandas will show\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "## Display some of the documents (tweets)\n",
    "df[['text-raw']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a104993b-4913-49ba-a9d5-c15177e181ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[df['text'].str.contains('http://')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935409bd-a7fb-4100-b043-59846d065328",
   "metadata": {},
   "source": [
    "We can see here that there are some URLs in the text.  This will be a problem for normalization.  We will remove those."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930cd2bd-5fbf-4d38-b932-05ae2de1e25e",
   "metadata": {},
   "source": [
    "## Get some statistics on the length of **documents**\n",
    "\n",
    "Let's see how long each tweet is and determine the average length of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e65b4f6-78ec-4344-aca0-83c43a78336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Determine the length of each tweet\n",
    "## Create a new column of the lengths of each tweet\n",
    "df['length']= df['text-raw'].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62170df2-9dda-479d-b003-37a401889bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analyze the statistics of the lengths\n",
    "df['length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5dc423-860b-44ec-925d-cccb156e1cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New \n",
    "import seaborn as sns\n",
    "ax = sns.histplot(data=df, x='length')\n",
    "#ax.axvline(df['length'].mean(),color='k',ls=':');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8ea71b-5350-41f3-906b-505413c0c5a9",
   "metadata": {},
   "source": [
    "<a name='regex'></a>\n",
    "## ✨ Removing HTML From Reviews with Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2a8c4a-460a-4df2-ab3c-50e076ab94c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['text-raw'].str.contains('http://')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e198d1ec-b4b6-44c3-a74d-55b98cabba99",
   "metadata": {},
   "source": [
    "- Regular expression figured out with Google Bard: https://g.co/bard/share/1db36656cbf3\n",
    "\n",
    "- Tested out in this saved regex101 pattern: - https://regex101.com/r/01bd7q/3  with the values from below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc0073a-9345-4c50-a746-12d22c75adce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for raw html\n",
    "df.loc[df['text-raw'].str.contains('<')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768fff2e-2218-4c8b-9f9a-6d4399d8d26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Copy/pasted these values into regex 101\n",
    "# df.loc[df['text-raw'].str.contains('<')]['text-raw'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a0ab3f-b1d3-4dca-9235-011255b82137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Regular expression to match HTML tags\n",
    "regex = r\"<[^>]*>\"\n",
    "\n",
    "# Apply the regex to the DataFrame column using str.replace\n",
    "df['text'] = df['text-raw'].str.replace(regex, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1505f289-68ca-4fd0-9756-0bbfa6f099ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2701ac-dc9e-4692-bb21-7c1fedf85f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.loc[df['text-raw'].str.contains('<'),['text-raw', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a353e83-1ad9-4c84-a785-c14feef29029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f8f7818-5a2e-48f2-87bf-29678b7f9be6",
   "metadata": {},
   "source": [
    "# Text Normalization with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a869f6-073e-4281-b21a-33aba2fdd553",
   "metadata": {},
   "source": [
    "## Normalizing Casing\n",
    "\n",
    "It's common practice to lower the casing of the text in our documents to contribut to normalizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550db04b-32a7-4270-9420-e6da4eaadb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lower the casing of each document\n",
    "df['lower_text'] = df['text'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b174777b-b52a-4260-b486-b75ec1a2ba0f",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "Tokenizing text into single word tokens is simple in Python.  We can just use `str.split()`.  The default separator for `.split()` is one space, so `' '`.\n",
    "\n",
    "We can access Pandas' string accessor with `df.str.<method>`.  This allows us to apply string methods to all rows in a column.\n",
    "\n",
    "When processing text, if memory allows, it can be useful to keep many versions of your text: tokenize, lemmatized, no stop words, etc.  Some analysis or modeling packages expect tokenized data and others do not.  We often want to use different versions for different kinds of analysis, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b244d34-3f00-4b0a-af13-d9f0ee324398",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the documents into tokens\n",
    "\n",
    "df['tokens'] = df['lower_text'].str.split()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68391d27-b08b-469a-b5cc-c2571b849b14",
   "metadata": {},
   "source": [
    "### Better way to tokenize data\n",
    "\n",
    "NLTK has a more sophisticated tokenization function that will isolate things like punctuation as well.  This way 'hooray' and 'hooray!!!' will be the same token.\n",
    "\n",
    "In order for NLTK to recognize the punctuation, we will need to download the 'punkt' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cb8036-ffbe-4365-aacc-3ee5cc5e05e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download punkt\n",
    "nltk.download('punkt')\n",
    "\n",
    "## Tokenize with nltk.word_tokenize instead\n",
    "\n",
    "df['tokens'] = df['lower_text'].apply(nltk.word_tokenize)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92f435f-e144-4b72-8c07-3249dae5b164",
   "metadata": {},
   "source": [
    "## Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e91158-047d-4ba7-9a6b-67ae623a33b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download NLTK stopword list\n",
    "nltk.download('stopwords')\n",
    "\n",
    "## Load the English stop words.\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b32bd5a-fe7d-4766-baec-55ceb0f2f78c",
   "metadata": {},
   "source": [
    "<font color=red> NOTICE </font> that all of the stop words are lower case.  It's necessary to ensure that your tokens are all lower case before using this list to remove stop words.\n",
    "\n",
    "To remove the stop words from each document, we will apply a function that will check each word in the list of tokens against the list of stopwords and remove them if they are in the list.  More specifically, it will only save them if they are NOT in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7cd190-07ee-40ee-b6c2-4c8639ff4cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create function to remove stop words\n",
    "def remove_stopwords(tokens):\n",
    "    # no_stops = []\n",
    "    # for token in tokens:\n",
    "    #     if token not in stop_words:\n",
    "    #         no_stops.append(token)\n",
    "\n",
    "    no_stops = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    return no_stops\n",
    "    \n",
    "## Apply the function to the tokenized data\n",
    "\n",
    "df['no_stops'] = df['tokens'].map(remove_stopwords)\n",
    "df.head(10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29e8234-98a9-4e5b-8994-71be25afc15d",
   "metadata": {},
   "source": [
    "## Remove Punctuation\n",
    "\n",
    "We can remove punctuation in a similar that we removed stop words.  However, we will get our list of punctuation from the built in Python string library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cb2857-8b5f-4c61-99f0-3a4151f0c0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import built-in String Libary\n",
    "from string import punctuation\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c02a29f-39f3-4671-93f8-0452c223fe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create function to remove punctuation tokens\n",
    "\n",
    "def remove_punct(tokens):\n",
    "    no_punct = []\n",
    "    for token in tokens:\n",
    "        if token not in punctuation:\n",
    "            no_punct.append(token)\n",
    "    return no_punct\n",
    "\n",
    "## Apply the function to the tokens without punctuation\n",
    "\n",
    "df['no_stops_no_punct'] = df['no_stops'].apply(remove_punct)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de4d7d9-b014-493c-90f3-26af7bbbc93a",
   "metadata": {},
   "source": [
    "## Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8f40a1-9c75-4aca-be8e-b14d370a302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## [v3 For Loop - Continue] Define function to remove URLs\n",
    "def remove_urls(token_list):\n",
    "    no_urls = []\n",
    "    for token in token_list:\n",
    "        if ('http' in token) | ('www' in token):\n",
    "            continue\n",
    "        no_urls.append(token)\n",
    "    return no_urls\n",
    "\n",
    "## Remove URLs from no_stops_no_punct\n",
    "df['no_stops_no_punct'] = df['no_stops_no_punct'].apply(remove_urls)\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beac0458-1764-4f82-ba99-0dd7ee47a16e",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Note how many fewer tokens we have in our `no_stops_no_punct` tokens than in our original.  However, some information was lost, but a lot was also retained.  \n",
    "\n",
    "Normalization is a huge part of the NLP process and is always a balance between reducing the size of our vocabulary and therefor simplifying our models, and retaining enough information for the model to extract some meaningful patterns in the texts.  \n",
    "\n",
    "There are a lot of choices here to make."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ed88e6-d9b1-4a54-943c-76d26209f01b",
   "metadata": {},
   "source": [
    "# Normalizing Text with spaCy\n",
    "\n",
    "The spaCy Python package provides text processing pipelines that can do many of these operations, plus much more complicated processing, very fast and in many fewer steps.  For this reason it is a very popular tool.  \n",
    "\n",
    "It utilizes pretrained language models that can recognize things like parts of speech and named entities (people, specific places, currency, etc.)\n",
    "\n",
    "spaCy was not included in your original dojo_env, so you will need to install if if you have not already.\n",
    "\n",
    "We will also download the pretrained english language model trained on millions of web documents.  We will use the small sized one for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3e87dc-befc-4cc1-b970-75fcac2026fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install spacy if necessary\n",
    "#!pip install spacy\n",
    "\n",
    "import spacy\n",
    "\n",
    "## Download the English small-sized model trained on web documents if necessary\n",
    "# spacy.cli.download('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5a50f2",
   "metadata": {},
   "source": [
    "## The spaCy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416eaea3-add2-481f-b77e-79c67901f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the model.  Disable Named Entity Recognizer (too slow)\n",
    "nlp_model = spacy.load('en_core_web_sm', disable='ner')\n",
    "\n",
    "## Display the names of each tranformer pipe\n",
    "nlp_model.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec96eaf",
   "metadata": {},
   "source": [
    "We have our model, and we can apply it like a function.  It expects a string of text as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2547ee99-d535-46ba-88a6-ea83fa764199",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c6de9a-f702-48cb-9f19-369b07473ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New\n",
    "idx_example = 286#2873\n",
    "raw_text = df.loc[idx_example,'text']\n",
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0f4536",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process a document with the model\n",
    "doc = nlp_model(raw_text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7015e44-6c9b-43ab-a9f6-1022bfb57d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['text'][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dbdb73-404f-4c4f-ba6b-92ca8eac1a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_model(df['text'][20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856a8134",
   "metadata": {},
   "source": [
    "**The document is a collection of tokens we can iterate over**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca267ae",
   "metadata": {},
   "source": [
    "## Documents and Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7e5a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the tokens in the document\n",
    "\n",
    "[token for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e69343c",
   "metadata": {},
   "source": [
    "Each token is much more than a string.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b189b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Isolate the last token in the document\n",
    "doc = nlp_model(\"I thought I did my homework but I forgot I was running late and didn't finish.\")\n",
    "word = doc[-4]\n",
    "\n",
    "## Display the text and type of the token\n",
    "print(word)\n",
    "type(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccad5b18",
   "metadata": {},
   "source": [
    "Each has many attributes that we can take advantage of, such as the lemma form and whether it is punctuation or space, and whether it is a stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9552bd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the lemmatized form of the token\n",
    "\n",
    "word.lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2938ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check whether the token is punctuation\n",
    "word.is_punct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30913fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check whether the token is a space\n",
    "word.is_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5ab785",
   "metadata": {},
   "source": [
    "Spacy can even determine the part of speech that the token is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c38600",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the part of speech of the token\n",
    "word.pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a81063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show the parts of speech for each token in the document\n",
    "\n",
    "[token.pos_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875e6dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show a list of the lemmas for each token in the document\n",
    "\n",
    "[token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06915d48",
   "metadata": {},
   "source": [
    "Notice that spaCy does not lower the case of lemmas.  Let's make sure we do that, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa373c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show a list of only the tokens in the document that are not punctuation or spaces or URLs\n",
    "lemmas_list = []\n",
    "for token in doc:\n",
    "    if token.is_punct:\n",
    "        continue\n",
    "    if token.is_space:\n",
    "        continue\n",
    "    if token.is_stop:\n",
    "        continue\n",
    "\n",
    "    lemmas_list.append(token.lemma_.lower())\n",
    "\n",
    "lemmas_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c578e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show a list of all the tokens in the document that are not punctuation, spaces, or stop words\n",
    "[token.lemma_.lower() for token in doc if \n",
    " not token.is_punct and \n",
    " not token.is_space and \n",
    " not token.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d5eaac",
   "metadata": {},
   "source": [
    "In order to use spaCy to process our entire dataframe, we will need to make a function and apply it to our text column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e8936b-44b5-4b5b-a332-1fc90ba7c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's also remove URLs\n",
    "## Let's also remove the url\n",
    "[token.lemma_.lower() for token in doc if \n",
    " not token.is_punct and \n",
    " not token.is_space and \n",
    " not token.is_stop and \n",
    " not 'http' in token.lemma_.lower() and\n",
    " not 'www' in token.lemma_.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437115ec",
   "metadata": {},
   "source": [
    "## Preprocessing with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626c06aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a function to use spacy to process our text\n",
    "def spacy_process(text):\n",
    "        \"\"\"Lemmatize tokens, lower case, remove punctuation, spaces, and stop words\"\"\"\n",
    "        doc = nlp_model(text)\n",
    "        processed_doc = [token.lemma_.lower() \n",
    "                         for token in doc if not token.is_punct and \n",
    "                         not token.is_space and not token.is_stop and \n",
    "                         not 'http' in token.lemma_.lower() and 'www' not in token.lemma_.lower()]\n",
    "        return processed_doc\n",
    "\n",
    "## process the tweets using the spacy function\n",
    "df['spacy_lemmas'] = df['text'].apply(spacy_process)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8ce1f3",
   "metadata": {},
   "source": [
    "We used spaCy to tokenize, lemmatize, and remove punctuation and stopwords from our text in one step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086f35e6",
   "metadata": {},
   "source": [
    "Notice that the spaCy processed data is a little different than our previously processed data.  The text has been lemmatized and spaCy has a different list of stop words than NLTK.\n",
    "\n",
    "The learn platform has directions for how you can customize your spaCy stopword list and a function with more flexibility in how spaCy will process your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc6f5c2",
   "metadata": {},
   "source": [
    "# ngrams\n",
    "combine multiple words into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0413ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the ngrams function\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7064b8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Isolate the 6th lemmatized document\n",
    "lemma_doc = df['spacy_lemmas'][5]\n",
    "lemma_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f77bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create list of bigrams\n",
    "list(ngrams(lemma_doc,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5764e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create list of trigrams\n",
    "list(ngrams(lemma_doc,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cddca69",
   "metadata": {},
   "source": [
    "\n",
    "## Applying `ngrams` to make a new column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715de044",
   "metadata": {},
   "source": [
    "\n",
    "We need to make a function that returns a list of bigrams.  It won't work to just pass the ngrams function to `.apply()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4604ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a function to create bigrams\n",
    "def make_bigrams(doc):\n",
    "    bigrams = ngrams(doc, 2)\n",
    "    bigrams = list(bigrams)\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f23807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add bigrams to the df with .apply()\n",
    "df['bigrams'] = df['spacy_lemmas'].apply(make_bigrams)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5e8efd",
   "metadata": {},
   "source": [
    "\n",
    "# Save the final data version for modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdea059-de02-4f2e-a40d-ad5370e3f9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0998455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Save the processed data\n",
    "df.to_csv('../Data-AmazonReviews/processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144062b1-ad45-4ac8-b587-d26a0758d601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the processed data\n",
    "import joblib\n",
    "\n",
    "joblib.dump(df, '../Data-AmazonReviews/processed_data.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9875936d-5cf9-453d-b530-d173bcd2e03f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dojo-env)",
   "language": "python",
   "name": "dojo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
