{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f34a8583-3d57-4f07-8a10-9459ee3cdb09",
   "metadata": {},
   "source": [
    "Advanced ML Week 1, Lecture 1: Working with and Preparing Text Data\n",
    "\n",
    "In this notebook we will be preparing Twitter (X) Tweets for sentiment analysis.  Sentiment analysis is a common text classification challenge to determine whether a text is positive or negative.  \n",
    "\n",
    "This is useful for companies that want to analyze large numbers of documents, tweets, reviews, etc., to determine public sentiment about a product or service.\n",
    "\n",
    "The data was originally gathered from Twitter (now X) and hand-labeled.  Of course there will be some human bias in the labeling.  It was downloaded from Kaggle at this site: [Kaggle Twitter Tweets Sentiment Dataset](https://www.kaggle.com/datasets/yasserh/twitter-tweets-sentiment-dataset/)\n",
    "\n",
    "There are 3 classes: positive, negative, and neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a6c695c-5cfd-4069-aacf-dacd0457e3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary packages\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c0645a-3d8a-41f9-a678-23b5862ac8a2",
   "metadata": {},
   "source": [
    "# Load the Data\n",
    "\n",
    "We will download our **corpus** of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f666473-51b7-4c60-8ce6-ae94e5b3d73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Download corpus of tweets\n",
    "df = pd.read_csv('../Data/archive.zip')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15206203-a97b-448d-b5e9-a95c511ec56a",
   "metadata": {},
   "source": [
    "# Some light EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12e60c31-28a8-48a2-a150-cc2eb9ed3f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27481 entries, 0 to 27480\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   textID         27481 non-null  object\n",
      " 1   text           27480 non-null  object\n",
      " 2   selected_text  27480 non-null  object\n",
      " 3   sentiment      27481 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 858.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc3e96d1-975b-493b-9551-9e242871df7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafde953-8792-47b9-861e-3f53b6e3a177",
   "metadata": {},
   "source": [
    "# Some Light Data Cleaning\n",
    "\n",
    "We see that our **corpus** has 27481 **documents**, each with an ID, the full text, a shortened version, and the labeled sentiment.\n",
    "\n",
    "Interestingly, one of the tweets has no text!  We definitely want to get rid of that.  We will also drop the `textID` and `selected_text` columns.  We are going to use the entire text of each tweet, not just a subset.\n",
    "\n",
    "We will keep the label, `sentiment` for later classification and analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64be5747-3e29-4745-ab38-c25e09255dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['textID', 'selected_text'])\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbbbcd7d-1e2f-4d44-bb98-43ece33d3ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 27480 entries, 0 to 27480\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   text       27480 non-null  object\n",
      " 1   sentiment  27480 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 644.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922b390c-c90c-46d4-be9f-061e8a66f2f2",
   "metadata": {},
   "source": [
    "# Some More EDA\n",
    "Let's look at some aspects of this text.\n",
    "* What do the **documents** look like?\n",
    "* How long do the tend to be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0656c034-5ce8-4214-8147-a18f1150699f",
   "metadata": {},
   "source": [
    "## View some sample tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f2e8add-e130-49ac-8ffa-9dcb7d59568d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                             I`d have responded, if I were going\n",
       "1                                                   Sooo SAD I will miss you here in San Diego!!!\n",
       "2                                                                       my boss is bullying me...\n",
       "3                                                                  what interview! leave me alone\n",
       "4                      Sons of ****, why couldn`t they put them on the releases we already bought\n",
       "5    http://www.dothebouncy.com/smf - some shameless plugging for the best Rangers forum on earth\n",
       "6                                2am feedings for the baby are fun when he is all smiles and coos\n",
       "7                                                                                      Soooo high\n",
       "8                                                                                     Both of you\n",
       "9                            Journey!? Wow... u just became cooler.  hehe... (is that possible!?)\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Expand how many characters pandas will show\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "## Display some of the documents (tweets)\n",
    "df['text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a104993b-4913-49ba-a9d5-c15177e181ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameless plugging for the best Rangers forum on earth</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>i`ve been sick for the past few days  and thus, my hair looks wierd.  if i didnt have a hat on it would look... http://tinyurl.com/mnf4kw</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Thats it, its the end. Tears for Fears vs Eric Prydz, DJ Hero   http://bit.ly/2Hpbg4</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Then you should check out http://twittersucks.com and connect with other tweeple who hate twitter</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>will be back later.  http://plurk.com/p/rp3k7</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27374</th>\n",
       "      <td>says Finally, Im home.  http://plurk.com/p/rr121</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27384</th>\n",
       "      <td>This is a much better tool than some I have come across http://www.tweepular.com - Twitter Karma on Steroids</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27386</th>\n",
       "      <td>#vwll2009 Would one of the VWLLers want to add this event to our Ning?   http://bit.ly/BF5sh  Would much appreciate that</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27463</th>\n",
       "      <td>LIKE DREW SAID 'GIVE TC A CHANCE' WE WILL MISS THOMAS  BUT HAVE TO MOVE ON. SO WATCH THIS! http://bit.ly/r6RfC</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27472</th>\n",
       "      <td>http://twitpic.com/663vr - Wanted to visit the animals but we were too late</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1220 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                            text  \\\n",
       "5                                                   http://www.dothebouncy.com/smf - some shameless plugging for the best Rangers forum on earth   \n",
       "17     i`ve been sick for the past few days  and thus, my hair looks wierd.  if i didnt have a hat on it would look... http://tinyurl.com/mnf4kw   \n",
       "35                                                          Thats it, its the end. Tears for Fears vs Eric Prydz, DJ Hero   http://bit.ly/2Hpbg4   \n",
       "50                                             Then you should check out http://twittersucks.com and connect with other tweeple who hate twitter   \n",
       "57                                                                                                 will be back later.  http://plurk.com/p/rp3k7   \n",
       "...                                                                                                                                          ...   \n",
       "27374                                                                                           says Finally, Im home.  http://plurk.com/p/rr121   \n",
       "27384                               This is a much better tool than some I have come across http://www.tweepular.com - Twitter Karma on Steroids   \n",
       "27386                   #vwll2009 Would one of the VWLLers want to add this event to our Ning?   http://bit.ly/BF5sh  Would much appreciate that   \n",
       "27463                             LIKE DREW SAID 'GIVE TC A CHANCE' WE WILL MISS THOMAS  BUT HAVE TO MOVE ON. SO WATCH THIS! http://bit.ly/r6RfC   \n",
       "27472                                                                http://twitpic.com/663vr - Wanted to visit the animals but we were too late   \n",
       "\n",
       "      sentiment  \n",
       "5       neutral  \n",
       "17     negative  \n",
       "35      neutral  \n",
       "50      neutral  \n",
       "57      neutral  \n",
       "...         ...  \n",
       "27374   neutral  \n",
       "27384  positive  \n",
       "27386  positive  \n",
       "27463  negative  \n",
       "27472  negative  \n",
       "\n",
       "[1220 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['text'].str.contains('http://')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935409bd-a7fb-4100-b043-59846d065328",
   "metadata": {},
   "source": [
    "We can see here that there are some URLs in the text.  This will be a problem for normalization.  We will remove those."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930cd2bd-5fbf-4d38-b932-05ae2de1e25e",
   "metadata": {},
   "source": [
    "## Get some statistics on the length of **documents**\n",
    "\n",
    "Let's see how long each tweet is and determine the average length of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e65b4f6-78ec-4344-aca0-83c43a78336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Determine the length of each tweet\n",
    "## Create a new column of the lengths of each tweet\n",
    "df['length']= df['text'].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62170df2-9dda-479d-b003-37a401889bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    27480.000000\n",
       "mean        68.330022\n",
       "std         35.603870\n",
       "min          3.000000\n",
       "25%         39.000000\n",
       "50%         64.000000\n",
       "75%         97.000000\n",
       "max        141.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Analyze the statistics of the lengths\n",
    "df['length'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45dafca-dc80-4cba-862d-e9ba98de19dd",
   "metadata": {},
   "source": [
    "The tweets have an mean length of 68 characters and a median of 64. They range from 3 to 141 characters with a standard deviation of 35.  The middle 50% are between 39 and 97 characters in length.\n",
    "\n",
    "This gives us some idea of how long they tend to be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8f7818-5a2e-48f2-87bf-29678b7f9be6",
   "metadata": {},
   "source": [
    "# Text Normalization with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a869f6-073e-4281-b21a-33aba2fdd553",
   "metadata": {},
   "source": [
    "## Normalizing Casing\n",
    "\n",
    "It's common practice to lower the casing of the text in our documents to contribut to normalizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "550db04b-32a7-4270-9420-e6da4eaadb1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>length</th>\n",
       "      <th>lower_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>36</td>\n",
       "      <td>i`d have responded, if i were going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>46</td>\n",
       "      <td>sooo sad i will miss you here in san diego!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>25</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>31</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>negative</td>\n",
       "      <td>75</td>\n",
       "      <td>sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          text  \\\n",
       "0                                          I`d have responded, if I were going   \n",
       "1                                Sooo SAD I will miss you here in San Diego!!!   \n",
       "2                                                    my boss is bullying me...   \n",
       "3                                               what interview! leave me alone   \n",
       "4   Sons of ****, why couldn`t they put them on the releases we already bought   \n",
       "\n",
       "  sentiment  length  \\\n",
       "0   neutral      36   \n",
       "1  negative      46   \n",
       "2  negative      25   \n",
       "3  negative      31   \n",
       "4  negative      75   \n",
       "\n",
       "                                                                    lower_text  \n",
       "0                                          i`d have responded, if i were going  \n",
       "1                                sooo sad i will miss you here in san diego!!!  \n",
       "2                                                    my boss is bullying me...  \n",
       "3                                               what interview! leave me alone  \n",
       "4   sons of ****, why couldn`t they put them on the releases we already bought  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Lower the casing of each document\n",
    "df['lower_text'] = df['text'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b174777b-b52a-4260-b486-b75ec1a2ba0f",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "Tokenizing text into single word tokens is simple in Python.  We can just use `str.split()`.  The default separator for `.split()` is one space, so `' '`.\n",
    "\n",
    "We can access Pandas' string accessor with `df.str.<method>`.  This allows us to apply string methods to all rows in a column.\n",
    "\n",
    "When processing text, if memory allows, it can be useful to keep many versions of your text: tokenize, lemmatized, no stop words, etc.  Some analysis or modeling packages expect tokenized data and others do not.  We often want to use different versions for different kinds of analysis, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b244d34-3f00-4b0a-af13-d9f0ee324398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>length</th>\n",
       "      <th>lower_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>36</td>\n",
       "      <td>i`d have responded, if i were going</td>\n",
       "      <td>[i`d, have, responded,, if, i, were, going]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>46</td>\n",
       "      <td>sooo sad i will miss you here in san diego!!!</td>\n",
       "      <td>[sooo, sad, i, will, miss, you, here, in, san, diego!!!]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>25</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>[my, boss, is, bullying, me...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>31</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>[what, interview!, leave, me, alone]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>negative</td>\n",
       "      <td>75</td>\n",
       "      <td>sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>[sons, of, ****,, why, couldn`t, they, put, them, on, the, releases, we, already, bought]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          text  \\\n",
       "0                                          I`d have responded, if I were going   \n",
       "1                                Sooo SAD I will miss you here in San Diego!!!   \n",
       "2                                                    my boss is bullying me...   \n",
       "3                                               what interview! leave me alone   \n",
       "4   Sons of ****, why couldn`t they put them on the releases we already bought   \n",
       "\n",
       "  sentiment  length  \\\n",
       "0   neutral      36   \n",
       "1  negative      46   \n",
       "2  negative      25   \n",
       "3  negative      31   \n",
       "4  negative      75   \n",
       "\n",
       "                                                                    lower_text  \\\n",
       "0                                          i`d have responded, if i were going   \n",
       "1                                sooo sad i will miss you here in san diego!!!   \n",
       "2                                                    my boss is bullying me...   \n",
       "3                                               what interview! leave me alone   \n",
       "4   sons of ****, why couldn`t they put them on the releases we already bought   \n",
       "\n",
       "                                                                                      tokens  \n",
       "0                                                [i`d, have, responded,, if, i, were, going]  \n",
       "1                                   [sooo, sad, i, will, miss, you, here, in, san, diego!!!]  \n",
       "2                                                            [my, boss, is, bullying, me...]  \n",
       "3                                                       [what, interview!, leave, me, alone]  \n",
       "4  [sons, of, ****,, why, couldn`t, they, put, them, on, the, releases, we, already, bought]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Split the documents into tokens\n",
    "\n",
    "df['tokens'] = df['lower_text'].str.split()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68391d27-b08b-469a-b5cc-c2571b849b14",
   "metadata": {},
   "source": [
    "### Better way to tokenize data\n",
    "\n",
    "NLTK has a more sophisticated tokenization function that will isolate things like punctuation as well.  This way 'hooray' and 'hooray!!!' will be the same token.\n",
    "\n",
    "In order for NLTK to recognize the punctuation, we will need to download the 'punkt' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99cb8036-ffbe-4365-aacc-3ee5cc5e05e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/purvikansara/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>length</th>\n",
       "      <th>lower_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>36</td>\n",
       "      <td>i`d have responded, if i were going</td>\n",
       "      <td>[i, `, d, have, responded, ,, if, i, were, going]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>46</td>\n",
       "      <td>sooo sad i will miss you here in san diego!!!</td>\n",
       "      <td>[sooo, sad, i, will, miss, you, here, in, san, diego, !, !, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>25</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>[my, boss, is, bullying, me, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>31</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>[what, interview, !, leave, me, alone]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>negative</td>\n",
       "      <td>75</td>\n",
       "      <td>sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>[sons, of, *, *, *, *, ,, why, couldn, `, t, they, put, them, on, the, releases, we, already, bought]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          text  \\\n",
       "0                                          I`d have responded, if I were going   \n",
       "1                                Sooo SAD I will miss you here in San Diego!!!   \n",
       "2                                                    my boss is bullying me...   \n",
       "3                                               what interview! leave me alone   \n",
       "4   Sons of ****, why couldn`t they put them on the releases we already bought   \n",
       "\n",
       "  sentiment  length  \\\n",
       "0   neutral      36   \n",
       "1  negative      46   \n",
       "2  negative      25   \n",
       "3  negative      31   \n",
       "4  negative      75   \n",
       "\n",
       "                                                                    lower_text  \\\n",
       "0                                          i`d have responded, if i were going   \n",
       "1                                sooo sad i will miss you here in san diego!!!   \n",
       "2                                                    my boss is bullying me...   \n",
       "3                                               what interview! leave me alone   \n",
       "4   sons of ****, why couldn`t they put them on the releases we already bought   \n",
       "\n",
       "                                                                                                  tokens  \n",
       "0                                                      [i, `, d, have, responded, ,, if, i, were, going]  \n",
       "1                                         [sooo, sad, i, will, miss, you, here, in, san, diego, !, !, !]  \n",
       "2                                                                      [my, boss, is, bullying, me, ...]  \n",
       "3                                                                 [what, interview, !, leave, me, alone]  \n",
       "4  [sons, of, *, *, *, *, ,, why, couldn, `, t, they, put, them, on, the, releases, we, already, bought]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Download punkt\n",
    "nltk.download('punkt')\n",
    "\n",
    "## Tokenize with nltk.word_tokenize instead\n",
    "\n",
    "df['tokens'] = df['lower_text'].apply(nltk.word_tokenize)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92f435f-e144-4b72-8c07-3249dae5b164",
   "metadata": {},
   "source": [
    "## Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32e91158-047d-4ba7-9a6b-67ae623a33b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/purvikansara/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Download NLTK stopword list\n",
    "nltk.download('stopwords')\n",
    "\n",
    "## Load the English stop words.\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b32bd5a-fe7d-4766-baec-55ceb0f2f78c",
   "metadata": {},
   "source": [
    "<font color=red> NOTICE </font> that all of the stop words are lower case.  It's necessary to ensure that your tokens are all lower case before using this list to remove stop words.\n",
    "\n",
    "To remove the stop words from each document, we will apply a function that will check each word in the list of tokens against the list of stopwords and remove them if they are in the list.  More specifically, it will only save them if they are NOT in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b7cd190-07ee-40ee-b6c2-4c8639ff4cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>length</th>\n",
       "      <th>lower_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>no_stops</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>36</td>\n",
       "      <td>i`d have responded, if i were going</td>\n",
       "      <td>[i, `, d, have, responded, ,, if, i, were, going]</td>\n",
       "      <td>[`, responded, ,, going]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>46</td>\n",
       "      <td>sooo sad i will miss you here in san diego!!!</td>\n",
       "      <td>[sooo, sad, i, will, miss, you, here, in, san, diego, !, !, !]</td>\n",
       "      <td>[sooo, sad, miss, san, diego, !, !, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>25</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>[my, boss, is, bullying, me, ...]</td>\n",
       "      <td>[boss, bullying, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>31</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>[what, interview, !, leave, me, alone]</td>\n",
       "      <td>[interview, !, leave, alone]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>negative</td>\n",
       "      <td>75</td>\n",
       "      <td>sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>[sons, of, *, *, *, *, ,, why, couldn, `, t, they, put, them, on, the, releases, we, already, bought]</td>\n",
       "      <td>[sons, *, *, *, *, ,, `, put, releases, already, bought]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameless plugging for the best Rangers forum on earth</td>\n",
       "      <td>neutral</td>\n",
       "      <td>92</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameless plugging for the best rangers forum on earth</td>\n",
       "      <td>[http, :, //www.dothebouncy.com/smf, -, some, shameless, plugging, for, the, best, rangers, forum, on, earth]</td>\n",
       "      <td>[http, :, //www.dothebouncy.com/smf, -, shameless, plugging, best, rangers, forum, earth]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2am feedings for the baby are fun when he is all smiles and coos</td>\n",
       "      <td>positive</td>\n",
       "      <td>64</td>\n",
       "      <td>2am feedings for the baby are fun when he is all smiles and coos</td>\n",
       "      <td>[2am, feedings, for, the, baby, are, fun, when, he, is, all, smiles, and, coos]</td>\n",
       "      <td>[2am, feedings, baby, fun, smiles, coos]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Soooo high</td>\n",
       "      <td>neutral</td>\n",
       "      <td>10</td>\n",
       "      <td>soooo high</td>\n",
       "      <td>[soooo, high]</td>\n",
       "      <td>[soooo, high]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Both of you</td>\n",
       "      <td>neutral</td>\n",
       "      <td>12</td>\n",
       "      <td>both of you</td>\n",
       "      <td>[both, of, you]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe... (is that possible!?)</td>\n",
       "      <td>positive</td>\n",
       "      <td>69</td>\n",
       "      <td>journey!? wow... u just became cooler.  hehe... (is that possible!?)</td>\n",
       "      <td>[journey, !, ?, wow, ..., u, just, became, cooler, ., hehe, ..., (, is, that, possible, !, ?, )]</td>\n",
       "      <td>[journey, !, ?, wow, ..., u, became, cooler, ., hehe, ..., (, possible, !, ?, )]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                           text  \\\n",
       "0                                                           I`d have responded, if I were going   \n",
       "1                                                 Sooo SAD I will miss you here in San Diego!!!   \n",
       "2                                                                     my boss is bullying me...   \n",
       "3                                                                what interview! leave me alone   \n",
       "4                    Sons of ****, why couldn`t they put them on the releases we already bought   \n",
       "5  http://www.dothebouncy.com/smf - some shameless plugging for the best Rangers forum on earth   \n",
       "6                              2am feedings for the baby are fun when he is all smiles and coos   \n",
       "7                                                                                    Soooo high   \n",
       "8                                                                                   Both of you   \n",
       "9                          Journey!? Wow... u just became cooler.  hehe... (is that possible!?)   \n",
       "\n",
       "  sentiment  length  \\\n",
       "0   neutral      36   \n",
       "1  negative      46   \n",
       "2  negative      25   \n",
       "3  negative      31   \n",
       "4  negative      75   \n",
       "5   neutral      92   \n",
       "6  positive      64   \n",
       "7   neutral      10   \n",
       "8   neutral      12   \n",
       "9  positive      69   \n",
       "\n",
       "                                                                                     lower_text  \\\n",
       "0                                                           i`d have responded, if i were going   \n",
       "1                                                 sooo sad i will miss you here in san diego!!!   \n",
       "2                                                                     my boss is bullying me...   \n",
       "3                                                                what interview! leave me alone   \n",
       "4                    sons of ****, why couldn`t they put them on the releases we already bought   \n",
       "5  http://www.dothebouncy.com/smf - some shameless plugging for the best rangers forum on earth   \n",
       "6                              2am feedings for the baby are fun when he is all smiles and coos   \n",
       "7                                                                                    soooo high   \n",
       "8                                                                                   both of you   \n",
       "9                          journey!? wow... u just became cooler.  hehe... (is that possible!?)   \n",
       "\n",
       "                                                                                                          tokens  \\\n",
       "0                                                              [i, `, d, have, responded, ,, if, i, were, going]   \n",
       "1                                                 [sooo, sad, i, will, miss, you, here, in, san, diego, !, !, !]   \n",
       "2                                                                              [my, boss, is, bullying, me, ...]   \n",
       "3                                                                         [what, interview, !, leave, me, alone]   \n",
       "4          [sons, of, *, *, *, *, ,, why, couldn, `, t, they, put, them, on, the, releases, we, already, bought]   \n",
       "5  [http, :, //www.dothebouncy.com/smf, -, some, shameless, plugging, for, the, best, rangers, forum, on, earth]   \n",
       "6                                [2am, feedings, for, the, baby, are, fun, when, he, is, all, smiles, and, coos]   \n",
       "7                                                                                                  [soooo, high]   \n",
       "8                                                                                                [both, of, you]   \n",
       "9               [journey, !, ?, wow, ..., u, just, became, cooler, ., hehe, ..., (, is, that, possible, !, ?, )]   \n",
       "\n",
       "                                                                                    no_stops  \n",
       "0                                                                   [`, responded, ,, going]  \n",
       "1                                                     [sooo, sad, miss, san, diego, !, !, !]  \n",
       "2                                                                      [boss, bullying, ...]  \n",
       "3                                                               [interview, !, leave, alone]  \n",
       "4                                   [sons, *, *, *, *, ,, `, put, releases, already, bought]  \n",
       "5  [http, :, //www.dothebouncy.com/smf, -, shameless, plugging, best, rangers, forum, earth]  \n",
       "6                                                   [2am, feedings, baby, fun, smiles, coos]  \n",
       "7                                                                              [soooo, high]  \n",
       "8                                                                                         []  \n",
       "9           [journey, !, ?, wow, ..., u, became, cooler, ., hehe, ..., (, possible, !, ?, )]  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create function to remove stop words\n",
    "def remove_stopwords(tokens):\n",
    "    # no_stops = []\n",
    "    # for token in tokens:\n",
    "    #     if token not in stop_words:\n",
    "    #         no_stops.append(token)\n",
    "\n",
    "    no_stops = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    return no_stops\n",
    "    \n",
    "## Apply the function to the tokenized data\n",
    "\n",
    "df['no_stops'] = df['tokens'].map(remove_stopwords)\n",
    "df.head(10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29e8234-98a9-4e5b-8994-71be25afc15d",
   "metadata": {},
   "source": [
    "## Remove Punctuation\n",
    "\n",
    "We can remove punctuation in a similar that we removed stop words.  However, we will get our list of punctuation from the built in Python string library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81cb2857-8b5f-4c61-99f0-3a4151f0c0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "## Import built-in String Libary\n",
    "from string import punctuation\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c02a29f-39f3-4671-93f8-0452c223fe61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>length</th>\n",
       "      <th>lower_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>no_stops</th>\n",
       "      <th>no_stops_no_punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>36</td>\n",
       "      <td>i`d have responded, if i were going</td>\n",
       "      <td>[i, `, d, have, responded, ,, if, i, were, going]</td>\n",
       "      <td>[`, responded, ,, going]</td>\n",
       "      <td>[responded, going]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>46</td>\n",
       "      <td>sooo sad i will miss you here in san diego!!!</td>\n",
       "      <td>[sooo, sad, i, will, miss, you, here, in, san, diego, !, !, !]</td>\n",
       "      <td>[sooo, sad, miss, san, diego, !, !, !]</td>\n",
       "      <td>[sooo, sad, miss, san, diego]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>25</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>[my, boss, is, bullying, me, ...]</td>\n",
       "      <td>[boss, bullying, ...]</td>\n",
       "      <td>[boss, bullying, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>31</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>[what, interview, !, leave, me, alone]</td>\n",
       "      <td>[interview, !, leave, alone]</td>\n",
       "      <td>[interview, leave, alone]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>negative</td>\n",
       "      <td>75</td>\n",
       "      <td>sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>[sons, of, *, *, *, *, ,, why, couldn, `, t, they, put, them, on, the, releases, we, already, bought]</td>\n",
       "      <td>[sons, *, *, *, *, ,, `, put, releases, already, bought]</td>\n",
       "      <td>[sons, put, releases, already, bought]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameless plugging for the best Rangers forum on earth</td>\n",
       "      <td>neutral</td>\n",
       "      <td>92</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameless plugging for the best rangers forum on earth</td>\n",
       "      <td>[http, :, //www.dothebouncy.com/smf, -, some, shameless, plugging, for, the, best, rangers, forum, on, earth]</td>\n",
       "      <td>[http, :, //www.dothebouncy.com/smf, -, shameless, plugging, best, rangers, forum, earth]</td>\n",
       "      <td>[http, //www.dothebouncy.com/smf, shameless, plugging, best, rangers, forum, earth]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2am feedings for the baby are fun when he is all smiles and coos</td>\n",
       "      <td>positive</td>\n",
       "      <td>64</td>\n",
       "      <td>2am feedings for the baby are fun when he is all smiles and coos</td>\n",
       "      <td>[2am, feedings, for, the, baby, are, fun, when, he, is, all, smiles, and, coos]</td>\n",
       "      <td>[2am, feedings, baby, fun, smiles, coos]</td>\n",
       "      <td>[2am, feedings, baby, fun, smiles, coos]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Soooo high</td>\n",
       "      <td>neutral</td>\n",
       "      <td>10</td>\n",
       "      <td>soooo high</td>\n",
       "      <td>[soooo, high]</td>\n",
       "      <td>[soooo, high]</td>\n",
       "      <td>[soooo, high]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Both of you</td>\n",
       "      <td>neutral</td>\n",
       "      <td>12</td>\n",
       "      <td>both of you</td>\n",
       "      <td>[both, of, you]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe... (is that possible!?)</td>\n",
       "      <td>positive</td>\n",
       "      <td>69</td>\n",
       "      <td>journey!? wow... u just became cooler.  hehe... (is that possible!?)</td>\n",
       "      <td>[journey, !, ?, wow, ..., u, just, became, cooler, ., hehe, ..., (, is, that, possible, !, ?, )]</td>\n",
       "      <td>[journey, !, ?, wow, ..., u, became, cooler, ., hehe, ..., (, possible, !, ?, )]</td>\n",
       "      <td>[journey, wow, ..., u, became, cooler, hehe, ..., possible]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                           text  \\\n",
       "0                                                           I`d have responded, if I were going   \n",
       "1                                                 Sooo SAD I will miss you here in San Diego!!!   \n",
       "2                                                                     my boss is bullying me...   \n",
       "3                                                                what interview! leave me alone   \n",
       "4                    Sons of ****, why couldn`t they put them on the releases we already bought   \n",
       "5  http://www.dothebouncy.com/smf - some shameless plugging for the best Rangers forum on earth   \n",
       "6                              2am feedings for the baby are fun when he is all smiles and coos   \n",
       "7                                                                                    Soooo high   \n",
       "8                                                                                   Both of you   \n",
       "9                          Journey!? Wow... u just became cooler.  hehe... (is that possible!?)   \n",
       "\n",
       "  sentiment  length  \\\n",
       "0   neutral      36   \n",
       "1  negative      46   \n",
       "2  negative      25   \n",
       "3  negative      31   \n",
       "4  negative      75   \n",
       "5   neutral      92   \n",
       "6  positive      64   \n",
       "7   neutral      10   \n",
       "8   neutral      12   \n",
       "9  positive      69   \n",
       "\n",
       "                                                                                     lower_text  \\\n",
       "0                                                           i`d have responded, if i were going   \n",
       "1                                                 sooo sad i will miss you here in san diego!!!   \n",
       "2                                                                     my boss is bullying me...   \n",
       "3                                                                what interview! leave me alone   \n",
       "4                    sons of ****, why couldn`t they put them on the releases we already bought   \n",
       "5  http://www.dothebouncy.com/smf - some shameless plugging for the best rangers forum on earth   \n",
       "6                              2am feedings for the baby are fun when he is all smiles and coos   \n",
       "7                                                                                    soooo high   \n",
       "8                                                                                   both of you   \n",
       "9                          journey!? wow... u just became cooler.  hehe... (is that possible!?)   \n",
       "\n",
       "                                                                                                          tokens  \\\n",
       "0                                                              [i, `, d, have, responded, ,, if, i, were, going]   \n",
       "1                                                 [sooo, sad, i, will, miss, you, here, in, san, diego, !, !, !]   \n",
       "2                                                                              [my, boss, is, bullying, me, ...]   \n",
       "3                                                                         [what, interview, !, leave, me, alone]   \n",
       "4          [sons, of, *, *, *, *, ,, why, couldn, `, t, they, put, them, on, the, releases, we, already, bought]   \n",
       "5  [http, :, //www.dothebouncy.com/smf, -, some, shameless, plugging, for, the, best, rangers, forum, on, earth]   \n",
       "6                                [2am, feedings, for, the, baby, are, fun, when, he, is, all, smiles, and, coos]   \n",
       "7                                                                                                  [soooo, high]   \n",
       "8                                                                                                [both, of, you]   \n",
       "9               [journey, !, ?, wow, ..., u, just, became, cooler, ., hehe, ..., (, is, that, possible, !, ?, )]   \n",
       "\n",
       "                                                                                    no_stops  \\\n",
       "0                                                                   [`, responded, ,, going]   \n",
       "1                                                     [sooo, sad, miss, san, diego, !, !, !]   \n",
       "2                                                                      [boss, bullying, ...]   \n",
       "3                                                               [interview, !, leave, alone]   \n",
       "4                                   [sons, *, *, *, *, ,, `, put, releases, already, bought]   \n",
       "5  [http, :, //www.dothebouncy.com/smf, -, shameless, plugging, best, rangers, forum, earth]   \n",
       "6                                                   [2am, feedings, baby, fun, smiles, coos]   \n",
       "7                                                                              [soooo, high]   \n",
       "8                                                                                         []   \n",
       "9           [journey, !, ?, wow, ..., u, became, cooler, ., hehe, ..., (, possible, !, ?, )]   \n",
       "\n",
       "                                                                     no_stops_no_punct  \n",
       "0                                                                   [responded, going]  \n",
       "1                                                        [sooo, sad, miss, san, diego]  \n",
       "2                                                                [boss, bullying, ...]  \n",
       "3                                                            [interview, leave, alone]  \n",
       "4                                               [sons, put, releases, already, bought]  \n",
       "5  [http, //www.dothebouncy.com/smf, shameless, plugging, best, rangers, forum, earth]  \n",
       "6                                             [2am, feedings, baby, fun, smiles, coos]  \n",
       "7                                                                        [soooo, high]  \n",
       "8                                                                                   []  \n",
       "9                          [journey, wow, ..., u, became, cooler, hehe, ..., possible]  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create function to remove punctuation tokens\n",
    "\n",
    "def remove_punct(tokens):\n",
    "    no_punct = []\n",
    "    for token in tokens:\n",
    "        if token not in punctuation:\n",
    "            no_punct.append(token)\n",
    "    return no_punct\n",
    "\n",
    "## Apply the function to the tokens without punctuation\n",
    "\n",
    "df['no_stops_no_punct'] = df['no_stops'].apply(remove_punct)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de4d7d9-b014-493c-90f3-26af7bbbc93a",
   "metadata": {},
   "source": [
    "## Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba8f40a1-9c75-4aca-be8e-b14d370a302b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>length</th>\n",
       "      <th>lower_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>no_stops</th>\n",
       "      <th>no_stops_no_punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>36</td>\n",
       "      <td>i`d have responded, if i were going</td>\n",
       "      <td>[i, `, d, have, responded, ,, if, i, were, going]</td>\n",
       "      <td>[`, responded, ,, going]</td>\n",
       "      <td>[responded, going]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>46</td>\n",
       "      <td>sooo sad i will miss you here in san diego!!!</td>\n",
       "      <td>[sooo, sad, i, will, miss, you, here, in, san, diego, !, !, !]</td>\n",
       "      <td>[sooo, sad, miss, san, diego, !, !, !]</td>\n",
       "      <td>[sooo, sad, miss, san, diego]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>25</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>[my, boss, is, bullying, me, ...]</td>\n",
       "      <td>[boss, bullying, ...]</td>\n",
       "      <td>[boss, bullying, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>31</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>[what, interview, !, leave, me, alone]</td>\n",
       "      <td>[interview, !, leave, alone]</td>\n",
       "      <td>[interview, leave, alone]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>negative</td>\n",
       "      <td>75</td>\n",
       "      <td>sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>[sons, of, *, *, *, *, ,, why, couldn, `, t, they, put, them, on, the, releases, we, already, bought]</td>\n",
       "      <td>[sons, *, *, *, *, ,, `, put, releases, already, bought]</td>\n",
       "      <td>[sons, put, releases, already, bought]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameless plugging for the best Rangers forum on earth</td>\n",
       "      <td>neutral</td>\n",
       "      <td>92</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameless plugging for the best rangers forum on earth</td>\n",
       "      <td>[http, :, //www.dothebouncy.com/smf, -, some, shameless, plugging, for, the, best, rangers, forum, on, earth]</td>\n",
       "      <td>[http, :, //www.dothebouncy.com/smf, -, shameless, plugging, best, rangers, forum, earth]</td>\n",
       "      <td>[shameless, plugging, best, rangers, forum, earth]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2am feedings for the baby are fun when he is all smiles and coos</td>\n",
       "      <td>positive</td>\n",
       "      <td>64</td>\n",
       "      <td>2am feedings for the baby are fun when he is all smiles and coos</td>\n",
       "      <td>[2am, feedings, for, the, baby, are, fun, when, he, is, all, smiles, and, coos]</td>\n",
       "      <td>[2am, feedings, baby, fun, smiles, coos]</td>\n",
       "      <td>[2am, feedings, baby, fun, smiles, coos]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Soooo high</td>\n",
       "      <td>neutral</td>\n",
       "      <td>10</td>\n",
       "      <td>soooo high</td>\n",
       "      <td>[soooo, high]</td>\n",
       "      <td>[soooo, high]</td>\n",
       "      <td>[soooo, high]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Both of you</td>\n",
       "      <td>neutral</td>\n",
       "      <td>12</td>\n",
       "      <td>both of you</td>\n",
       "      <td>[both, of, you]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe... (is that possible!?)</td>\n",
       "      <td>positive</td>\n",
       "      <td>69</td>\n",
       "      <td>journey!? wow... u just became cooler.  hehe... (is that possible!?)</td>\n",
       "      <td>[journey, !, ?, wow, ..., u, just, became, cooler, ., hehe, ..., (, is, that, possible, !, ?, )]</td>\n",
       "      <td>[journey, !, ?, wow, ..., u, became, cooler, ., hehe, ..., (, possible, !, ?, )]</td>\n",
       "      <td>[journey, wow, ..., u, became, cooler, hehe, ..., possible]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                           text  \\\n",
       "0                                                           I`d have responded, if I were going   \n",
       "1                                                 Sooo SAD I will miss you here in San Diego!!!   \n",
       "2                                                                     my boss is bullying me...   \n",
       "3                                                                what interview! leave me alone   \n",
       "4                    Sons of ****, why couldn`t they put them on the releases we already bought   \n",
       "5  http://www.dothebouncy.com/smf - some shameless plugging for the best Rangers forum on earth   \n",
       "6                              2am feedings for the baby are fun when he is all smiles and coos   \n",
       "7                                                                                    Soooo high   \n",
       "8                                                                                   Both of you   \n",
       "9                          Journey!? Wow... u just became cooler.  hehe... (is that possible!?)   \n",
       "\n",
       "  sentiment  length  \\\n",
       "0   neutral      36   \n",
       "1  negative      46   \n",
       "2  negative      25   \n",
       "3  negative      31   \n",
       "4  negative      75   \n",
       "5   neutral      92   \n",
       "6  positive      64   \n",
       "7   neutral      10   \n",
       "8   neutral      12   \n",
       "9  positive      69   \n",
       "\n",
       "                                                                                     lower_text  \\\n",
       "0                                                           i`d have responded, if i were going   \n",
       "1                                                 sooo sad i will miss you here in san diego!!!   \n",
       "2                                                                     my boss is bullying me...   \n",
       "3                                                                what interview! leave me alone   \n",
       "4                    sons of ****, why couldn`t they put them on the releases we already bought   \n",
       "5  http://www.dothebouncy.com/smf - some shameless plugging for the best rangers forum on earth   \n",
       "6                              2am feedings for the baby are fun when he is all smiles and coos   \n",
       "7                                                                                    soooo high   \n",
       "8                                                                                   both of you   \n",
       "9                          journey!? wow... u just became cooler.  hehe... (is that possible!?)   \n",
       "\n",
       "                                                                                                          tokens  \\\n",
       "0                                                              [i, `, d, have, responded, ,, if, i, were, going]   \n",
       "1                                                 [sooo, sad, i, will, miss, you, here, in, san, diego, !, !, !]   \n",
       "2                                                                              [my, boss, is, bullying, me, ...]   \n",
       "3                                                                         [what, interview, !, leave, me, alone]   \n",
       "4          [sons, of, *, *, *, *, ,, why, couldn, `, t, they, put, them, on, the, releases, we, already, bought]   \n",
       "5  [http, :, //www.dothebouncy.com/smf, -, some, shameless, plugging, for, the, best, rangers, forum, on, earth]   \n",
       "6                                [2am, feedings, for, the, baby, are, fun, when, he, is, all, smiles, and, coos]   \n",
       "7                                                                                                  [soooo, high]   \n",
       "8                                                                                                [both, of, you]   \n",
       "9               [journey, !, ?, wow, ..., u, just, became, cooler, ., hehe, ..., (, is, that, possible, !, ?, )]   \n",
       "\n",
       "                                                                                    no_stops  \\\n",
       "0                                                                   [`, responded, ,, going]   \n",
       "1                                                     [sooo, sad, miss, san, diego, !, !, !]   \n",
       "2                                                                      [boss, bullying, ...]   \n",
       "3                                                               [interview, !, leave, alone]   \n",
       "4                                   [sons, *, *, *, *, ,, `, put, releases, already, bought]   \n",
       "5  [http, :, //www.dothebouncy.com/smf, -, shameless, plugging, best, rangers, forum, earth]   \n",
       "6                                                   [2am, feedings, baby, fun, smiles, coos]   \n",
       "7                                                                              [soooo, high]   \n",
       "8                                                                                         []   \n",
       "9           [journey, !, ?, wow, ..., u, became, cooler, ., hehe, ..., (, possible, !, ?, )]   \n",
       "\n",
       "                                             no_stops_no_punct  \n",
       "0                                           [responded, going]  \n",
       "1                                [sooo, sad, miss, san, diego]  \n",
       "2                                        [boss, bullying, ...]  \n",
       "3                                    [interview, leave, alone]  \n",
       "4                       [sons, put, releases, already, bought]  \n",
       "5           [shameless, plugging, best, rangers, forum, earth]  \n",
       "6                     [2am, feedings, baby, fun, smiles, coos]  \n",
       "7                                                [soooo, high]  \n",
       "8                                                           []  \n",
       "9  [journey, wow, ..., u, became, cooler, hehe, ..., possible]  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## [v3 For Loop - Continue] Define function to remove URLs\n",
    "def remove_urls(token_list):\n",
    "    no_urls = []\n",
    "    for token in token_list:\n",
    "        if ('http' in token) | ('www' in token):\n",
    "            continue\n",
    "        no_urls.append(token)\n",
    "    return no_urls\n",
    "\n",
    "## Remove URLs from no_stops_no_punct\n",
    "df['no_stops_no_punct'] = df['no_stops_no_punct'].apply(remove_urls)\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beac0458-1764-4f82-ba99-0dd7ee47a16e",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Note how many fewer tokens we have in our `no_stops_no_punct` tokens than in our original.  However, some information was lost, but a lot was also retained.  \n",
    "\n",
    "Normalization is a huge part of the NLP process and is always a balance between reducing the size of our vocabulary and therefor simplifying our models, and retaining enough information for the model to extract some meaningful patterns in the texts.  \n",
    "\n",
    "There are a lot of choices here to make."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ed88e6-d9b1-4a54-943c-76d26209f01b",
   "metadata": {},
   "source": [
    "# Normalizing Text with spaCy\n",
    "\n",
    "The spaCy Python package provides text processing pipelines that can do many of these operations, plus much more complicated processing, very fast and in many fewer steps.  For this reason it is a very popular tool.  \n",
    "\n",
    "It utilizes pretrained language models that can recognize things like parts of speech and named entities (people, specific places, currency, etc.)\n",
    "\n",
    "spaCy was not included in your original dojo_env, so you will need to install if if you have not already.\n",
    "\n",
    "We will also download the pretrained english language model trained on millions of web documents.  We will use the small sized one for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a3e87dc-befc-4cc1-b970-75fcac2026fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.65.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (5.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.4)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (66.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.29.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.8.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.11.17)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dojo-env/lib/python3.10/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.1)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "## Install spacy if necessary\n",
    "#!pip install spacy\n",
    "\n",
    "import spacy\n",
    "\n",
    "## Download the English small-sized model trained on web documents if necessary\n",
    "spacy.cli.download('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5a50f2",
   "metadata": {},
   "source": [
    "## The spaCy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "416eaea3-add2-481f-b77e-79c67901f2ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the model.  Disable Named Entity Recognizer (too slow)\n",
    "nlp_model = spacy.load('en_core_web_sm', disable='ner')\n",
    "\n",
    "## Display the names of each tranformer pipe\n",
    "nlp_model.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec96eaf",
   "metadata": {},
   "source": [
    "We have our model, and we can apply it like a function.  It expects a string of text as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7c6de9a-f702-48cb-9f19-369b07473ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.dothebouncy.com/smf - some shameless plugging for the best Rangers forum on earth'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc0f4536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "http://www.dothebouncy.com/smf - some shameless plugging for the best Rangers forum on earth"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Process a document with the model\n",
    "\n",
    "doc = nlp_model(df['text'][5])\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856a8134",
   "metadata": {},
   "source": [
    "The document is a collection of tokens we can iterate over"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca267ae",
   "metadata": {},
   "source": [
    "## Documents and Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c7e5a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[http://www.dothebouncy.com/smf,\n",
       " -,\n",
       " some,\n",
       " shameless,\n",
       " plugging,\n",
       " for,\n",
       " the,\n",
       " best,\n",
       " Rangers,\n",
       " forum,\n",
       " on,\n",
       " earth]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Display the tokens in the document\n",
    "\n",
    "[token for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e69343c",
   "metadata": {},
   "source": [
    "Each token is much more than a string.  It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4b189b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Isolate the last token in the document\n",
    "word = doc[-1]\n",
    "\n",
    "## Display the text and type of the token\n",
    "print(word)\n",
    "type(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccad5b18",
   "metadata": {},
   "source": [
    "Each has many attributes that we can take advantage of, such as the lemma form and whether it is punctuation or space, and whether it is a stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9552bd2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'earth'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Display the lemmatized form of the token\n",
    "\n",
    "word.lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2938ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check whether the token is punctuation\n",
    "word.is_punct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e30913fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check whether the token is a space\n",
    "word.is_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5ab785",
   "metadata": {},
   "source": [
    "Spacy can even determine the part of speech that the token is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24c38600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOUN'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check the part of speech of the token\n",
    "word.pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a81063d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PROPN',\n",
       " 'PUNCT',\n",
       " 'DET',\n",
       " 'ADJ',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'ADJ',\n",
       " 'PROPN',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'NOUN']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Show the parts of speech for each token in the document\n",
    "\n",
    "[token.pos_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "875e6dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.dothebouncy.com/smf',\n",
       " '-',\n",
       " 'some',\n",
       " 'shameless',\n",
       " 'plug',\n",
       " 'for',\n",
       " 'the',\n",
       " 'good',\n",
       " 'Rangers',\n",
       " 'forum',\n",
       " 'on',\n",
       " 'earth']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Show a list of the lemmas for each token in the document\n",
    "\n",
    "[token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06915d48",
   "metadata": {},
   "source": [
    "Notice that spaCy does not lower the case of lemmas.  Let's make sure we do that, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa373c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.dothebouncy.com/smf',\n",
       " 'shameless',\n",
       " 'plug',\n",
       " 'good',\n",
       " 'rangers',\n",
       " 'forum',\n",
       " 'earth']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Show a list of only the tokens in the document that are not punctuation or spaces or URLs\n",
    "lemmas_list = []\n",
    "for token in doc:\n",
    "    if token.is_punct:\n",
    "        continue\n",
    "    if token.is_space:\n",
    "        continue\n",
    "    if token.is_stop:\n",
    "        continue\n",
    "\n",
    "    lemmas_list.append(token.lemma_.lower())\n",
    "\n",
    "lemmas_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c578e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show a list of all the tokens in the document that are not punctuation, spaces, or stop words\n",
    "[token.lemma_.lower() for token in doc if \n",
    " not token.is_punct and \n",
    " not token.is_space and \n",
    " not token.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d5eaac",
   "metadata": {},
   "source": [
    "In order to use spaCy to process our entire dataframe, we will need to make a function and apply it to our text column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "42e8936b-44b5-4b5b-a332-1fc90ba7c7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shameless', 'plug', 'good', 'rangers', 'forum', 'earth']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's also remove URLs\n",
    "## Let's also remove the url\n",
    "[token.lemma_.lower() for token in doc if \n",
    " not token.is_punct and \n",
    " not token.is_space and \n",
    " not token.is_stop and \n",
    " not 'http' in token.lemma_.lower() and\n",
    " not 'www' in token.lemma_.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437115ec",
   "metadata": {},
   "source": [
    "## Preprocessing with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "626c06aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>length</th>\n",
       "      <th>lower_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>no_stops</th>\n",
       "      <th>no_stops_no_punct</th>\n",
       "      <th>spacy_lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>36</td>\n",
       "      <td>i`d have responded, if i were going</td>\n",
       "      <td>[i, `, d, have, responded, ,, if, i, were, going]</td>\n",
       "      <td>[`, responded, ,, going]</td>\n",
       "      <td>[responded, going]</td>\n",
       "      <td>[i`d, respond, go]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>46</td>\n",
       "      <td>sooo sad i will miss you here in san diego!!!</td>\n",
       "      <td>[sooo, sad, i, will, miss, you, here, in, san, diego, !, !, !]</td>\n",
       "      <td>[sooo, sad, miss, san, diego, !, !, !]</td>\n",
       "      <td>[sooo, sad, miss, san, diego]</td>\n",
       "      <td>[sooo, sad, miss, san, diego]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>25</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>[my, boss, is, bullying, me, ...]</td>\n",
       "      <td>[boss, bullying, ...]</td>\n",
       "      <td>[boss, bullying, ...]</td>\n",
       "      <td>[boss, bully]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>31</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>[what, interview, !, leave, me, alone]</td>\n",
       "      <td>[interview, !, leave, alone]</td>\n",
       "      <td>[interview, leave, alone]</td>\n",
       "      <td>[interview, leave]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>negative</td>\n",
       "      <td>75</td>\n",
       "      <td>sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>[sons, of, *, *, *, *, ,, why, couldn, `, t, they, put, them, on, the, releases, we, already, bought]</td>\n",
       "      <td>[sons, *, *, *, *, ,, `, put, releases, already, bought]</td>\n",
       "      <td>[sons, put, releases, already, bought]</td>\n",
       "      <td>[son, couldn`t, release, buy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          text  \\\n",
       "0                                          I`d have responded, if I were going   \n",
       "1                                Sooo SAD I will miss you here in San Diego!!!   \n",
       "2                                                    my boss is bullying me...   \n",
       "3                                               what interview! leave me alone   \n",
       "4   Sons of ****, why couldn`t they put them on the releases we already bought   \n",
       "\n",
       "  sentiment  length  \\\n",
       "0   neutral      36   \n",
       "1  negative      46   \n",
       "2  negative      25   \n",
       "3  negative      31   \n",
       "4  negative      75   \n",
       "\n",
       "                                                                    lower_text  \\\n",
       "0                                          i`d have responded, if i were going   \n",
       "1                                sooo sad i will miss you here in san diego!!!   \n",
       "2                                                    my boss is bullying me...   \n",
       "3                                               what interview! leave me alone   \n",
       "4   sons of ****, why couldn`t they put them on the releases we already bought   \n",
       "\n",
       "                                                                                                  tokens  \\\n",
       "0                                                      [i, `, d, have, responded, ,, if, i, were, going]   \n",
       "1                                         [sooo, sad, i, will, miss, you, here, in, san, diego, !, !, !]   \n",
       "2                                                                      [my, boss, is, bullying, me, ...]   \n",
       "3                                                                 [what, interview, !, leave, me, alone]   \n",
       "4  [sons, of, *, *, *, *, ,, why, couldn, `, t, they, put, them, on, the, releases, we, already, bought]   \n",
       "\n",
       "                                                   no_stops  \\\n",
       "0                                  [`, responded, ,, going]   \n",
       "1                    [sooo, sad, miss, san, diego, !, !, !]   \n",
       "2                                     [boss, bullying, ...]   \n",
       "3                              [interview, !, leave, alone]   \n",
       "4  [sons, *, *, *, *, ,, `, put, releases, already, bought]   \n",
       "\n",
       "                        no_stops_no_punct                   spacy_lemmas  \n",
       "0                      [responded, going]             [i`d, respond, go]  \n",
       "1           [sooo, sad, miss, san, diego]  [sooo, sad, miss, san, diego]  \n",
       "2                   [boss, bullying, ...]                  [boss, bully]  \n",
       "3               [interview, leave, alone]             [interview, leave]  \n",
       "4  [sons, put, releases, already, bought]  [son, couldn`t, release, buy]  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Define a function to use spacy to process our text\n",
    "def spacy_process(text):\n",
    "        \"\"\"Lemmatize tokens, lower case, remove punctuation, spaces, and stop words\"\"\"\n",
    "        doc = nlp_model(text)\n",
    "        processed_doc = [token.lemma_.lower() \n",
    "                         for token in doc if not token.is_punct and \n",
    "                         not token.is_space and not token.is_stop and \n",
    "                         not 'http' in token.lemma_.lower() and 'www' not in token.lemma_.lower()]\n",
    "        return processed_doc\n",
    "\n",
    "## process the tweets using the spacy function\n",
    "df['spacy_lemmas'] = df['text'].apply(spacy_process)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8ce1f3",
   "metadata": {},
   "source": [
    "We used spaCy to tokenize, lemmatize, and remove punctuation and stopwords from our text in one step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086f35e6",
   "metadata": {},
   "source": [
    "Notice that the spaCy processed data is a little different than our previously processed data.  The text has been lemmatized and spaCy has a different list of stop words than NLTK.\n",
    "\n",
    "The learn platform has directions for how you can customize your spaCy stopword list and a function with more flexibility in how spaCy will process your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc6f5c2",
   "metadata": {},
   "source": [
    "# ngrams\n",
    "combine multiple words into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0413ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the ngrams function\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7064b8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Isolate the 6th lemmatized document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f77bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create list of bigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5764e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create list of trigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cddca69",
   "metadata": {},
   "source": [
    "\n",
    "## Applying `ngrams` to make a new column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715de044",
   "metadata": {},
   "source": [
    "\n",
    "We need to make a function that returns a list of bigrams.  It won't work to just pass the ngrams function to `.apply()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4604ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a function to create bigrams\n",
    "def make_bigrams(doc):\n",
    "    bigrams = ngrams(doc, 2)\n",
    "    bigrams = list(bigrams)\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f23807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add bigrams to the df with .apply()\n",
    "df['bigrams'] = df['spacy_lemmas'].apply(make_bigrams)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5e8efd",
   "metadata": {},
   "source": [
    "\n",
    "# Save the final data version for modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0998455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Save the processed data\n",
    "df.to_csv('../Data/processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "144062b1-ad45-4ac8-b587-d26a0758d601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Data/processed_data.joblib']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Save the processed data\n",
    "import joblib\n",
    "\n",
    "joblib.dump(df, '../Data/processed_data.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9875936d-5cf9-453d-b530-d173bcd2e03f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dojo-env)",
   "language": "python",
   "name": "dojo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
